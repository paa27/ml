{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:59:52.345560Z",
     "start_time": "2020-05-01T14:59:46.691307Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from random import choices\n",
    "from collections import deque\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    '''\n",
    "    Deep Q-Learning Network\n",
    "    \n",
    "    \n",
    "    Vocab:\n",
    "    ------\n",
    "    \n",
    "    observation: immediate info provided by environment\n",
    "    history: accumulation of state,action and observation\n",
    "    state: F(history) to provide always same input length\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,env,hyperparams=None):\n",
    "        \n",
    "        if hyperparams is None:\n",
    "            hyperparams = {}\n",
    "        \n",
    "        self.nactions = env.action_space.n\n",
    "        \n",
    "        self.epsilon = hyperparams.get('epsilon',0.5) #Exploration probability\n",
    "        self.epsilon_min = hyperparams.get('epsilon_min',0.1)\n",
    "        self.alpha = hyperparams.get('alpha',0.01) #Learning rate\n",
    "        self.gamma = hyperparams.get('gamma',1) #Discount rate\n",
    "        \n",
    "        self.nframes = hyperparams.get('nframes', 4)\n",
    "        \n",
    "        self.replay_capacity = int(hyperparams.get('N',10e2))\n",
    "        self.replay_memory = [] # Stores  {self.replay_capacity}x(state,action,reward,next_state)\n",
    "        \n",
    "        self.q = self._initialize_cnn()\n",
    "        self.batch_size = hyperparams.get('batch_size',32)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.SGD(self.q.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "        self.history = [] # Stores {self.nframes} most recent observations \n",
    "        \n",
    "\n",
    "    \n",
    "    def _initialize_cnn(self):\n",
    "        cnn = Net(channels_in = self.nframes, channels_out = self.nactions)\n",
    "        return cnn\n",
    "    \n",
    "    def memorize(self, action, reward, observation, done):\n",
    "        \n",
    "        phi = _phi(self.history, nframes = self.nframes)\n",
    "        self.history.append(observation)\n",
    "        next_phi = _phi(self.history, nframes = self.nframes)      \n",
    "        self.history = self.history[-10*self.nframes:] # 10 is simply an arbitrary history factor for debugging\n",
    "        \n",
    "        if (len(self.history) > self.nframes): \n",
    "            self.replay_memory.append([phi,\n",
    "                                       action,\n",
    "                                       reward,\n",
    "                                       next_phi,\n",
    "                                       done])\n",
    "        \n",
    "        self.replay_memory = self.replay_memory[-self.replay_capacity:]     \n",
    "    \n",
    "    def act(self):\n",
    "        '''Epsilon-greedy policy\n",
    "        '''\n",
    "        \n",
    "        if (len(self.history) >= self.nframes):\n",
    "            state = _phi(self.history, nframes = self.nframes)\n",
    "\n",
    "            if np.random.uniform(0,1)<self.epsilon:\n",
    "                action = int(np.random.choice(self.nactions))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = self.q(torch.unsqueeze(state, 0)).argmax().item()\n",
    "                    \n",
    "        else:\n",
    "            action = int(np.random.choice(self.nactions))\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        if (len(self.replay_memory) >= self.batch_size):\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            samples = choices(self.replay_memory, k = self.batch_size)\n",
    "\n",
    "            for sample in samples:\n",
    "                state, action, reward, next_state, done = sample\n",
    "                q_val = self.q(torch.unsqueeze(state, 0))\n",
    "                with torch.no_grad():\n",
    "                    target = self.q(torch.unsqueeze(state, 0))\n",
    "\n",
    "                    if done:\n",
    "                        target[0, action] = reward\n",
    "                    else:\n",
    "                        q_max = self.q(torch.unsqueeze(next_state, 0)).max().item()\n",
    "                        target[0, action] = reward + self.gamma*q_max\n",
    "\n",
    "                loss = self.criterion(q_val, target)\n",
    "                loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        \n",
    "    def _eval_q(self, numpy_state):\n",
    "        '''\n",
    "        For debugging only\n",
    "        '''\n",
    "        tensor_state = torch.unsqueeze(torch.from_numpy(numpy_state).float(), 0)\n",
    "        with torch.no_grad():\n",
    "            result = self.q(tensor_state)\n",
    "        return result.detach().numpy()\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, channels_in = 4, channels_out = 4):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels_in, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 17 * 17, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, channels_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 17 * 17)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def _preprocess_frame(frame):\n",
    "    '''\n",
    "    Takes RGB image and returns cnn input.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    frame: np.array, size: (210, 160, 3)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.array, size: (80,80)\n",
    "    \n",
    "    '''\n",
    "    greyscale = np.dot(frame[...,:3], [0.2989, 0.5870, 0.1140]) #(210,160,3) -> (210,160)\n",
    "    #Hard coded for breakout\n",
    "    downsampled = greyscale[::2,::2] #(210,160) -> (105,80)\n",
    "    squared = downsampled[17:97:,] #(105,80) -> (80,80) , Includes the full 'play area'\n",
    "    \n",
    "    return squared\n",
    "\n",
    "def _phi(history, nframes = 4):\n",
    "        '''Extracts current state from learned history\n",
    "        '''\n",
    "        frames = history[-nframes:]\n",
    "        state = torch.tensor([_preprocess_frame(frame) for frame in frames]).float()\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent,env,episodes):\n",
    "    \n",
    "    metrics = []\n",
    "    frames = []\n",
    "    \n",
    "    for episode in episodes:\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        agent.history.append(observation)\n",
    "        action = agent.act()\n",
    "        \n",
    "        while not done:\n",
    "            next_observation,done,reward,info = env.step(action)\n",
    "            next_action = agent.act()\n",
    "            agent.learn(observation,action,reward,next_observation)\n",
    "            \n",
    "            observation = next_observation\n",
    "            action = next_action\n",
    "            \n",
    "            update_metrics(metrics)\n",
    "            update_frames(frames)\n",
    "            \n",
    "    return metrics,frames\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T15:00:35.263149Z",
     "start_time": "2020-05-01T15:00:23.771967Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test init\n",
    "agent = DQN(env, hyperparams = {'nframes':1, 'batch_size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T15:01:50.001325Z",
     "start_time": "2020-05-01T15:01:36.394001Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, action 0\n",
      "Episode 0, action 5\n",
      "Episode 0, action 10\n",
      "Episode 0, action 15\n",
      "Episode 0, action 20\n",
      "Episode 0, action 25\n",
      "Episode 0, action 30\n",
      "Episode 0, action 35\n",
      "Episode 0, action 40\n",
      "Episode 0, action 45\n",
      "Episode 0, action 50\n",
      "Episode 0, action 55\n",
      "Episode 0, action 60\n",
      "Episode 0, action 65\n",
      "Episode 0, action 70\n",
      "Episode 0, action 75\n",
      "Episode 0, action 80\n",
      "Episode 0, action 85\n",
      "Episode 0, action 90\n",
      "Episode 0, action 95\n",
      "Episode 0, action 100\n",
      "Episode 0, action 105\n",
      "Episode 0, action 110\n",
      "Episode 0, action 115\n",
      "Episode 0, action 120\n",
      "Episode 0, action 125\n",
      "Episode 0, action 130\n",
      "Episode 0, action 135\n",
      "Episode 0, action 140\n",
      "Episode 0, action 145\n",
      "Episode 0, action 150\n",
      "Episode 0, action 155\n",
      "Episode 0, action 160\n",
      "Episode 0, action 165\n",
      "Episode 0, action 170\n",
      "Episode 0, action 175\n",
      "Episode 0, action 180\n",
      "Episode 0, action 185\n",
      "Episode 0, action 190\n",
      "Episode 1, action 0\n",
      "Episode 1, action 5\n",
      "Episode 1, action 10\n",
      "Episode 1, action 15\n",
      "Episode 1, action 20\n",
      "Episode 1, action 25\n",
      "Episode 1, action 30\n",
      "Episode 1, action 35\n",
      "Episode 1, action 40\n",
      "Episode 1, action 45\n",
      "Episode 1, action 50\n",
      "Episode 1, action 55\n",
      "Episode 1, action 60\n",
      "Episode 1, action 65\n",
      "Episode 1, action 70\n",
      "Episode 1, action 75\n",
      "Episode 1, action 80\n",
      "Episode 1, action 85\n",
      "Episode 1, action 90\n",
      "Episode 1, action 95\n",
      "Episode 1, action 100\n",
      "Episode 1, action 105\n",
      "Episode 1, action 110\n",
      "Episode 1, action 115\n",
      "Episode 1, action 120\n",
      "Episode 1, action 125\n",
      "Episode 1, action 130\n",
      "Episode 1, action 135\n",
      "Episode 1, action 140\n",
      "Episode 1, action 145\n",
      "Episode 1, action 150\n",
      "Episode 1, action 155\n",
      "Episode 1, action 160\n",
      "Episode 1, action 165\n",
      "Episode 1, action 170\n",
      "Episode 1, action 175\n",
      "Episode 1, action 180\n",
      "Episode 1, action 185\n",
      "Episode 1, action 190\n",
      "Episode 1, action 195\n",
      "Episode 1, action 200\n",
      "Episode 1, action 205\n",
      "Episode 1, action 210\n",
      "Episode 1, action 215\n",
      "Episode 1, action 220\n",
      "Episode 1, action 225\n",
      "Episode 1, action 230\n",
      "Episode 1, action 235\n",
      "Episode 1, action 240\n",
      "Episode 1, action 245\n",
      "Episode 1, action 250\n",
      "Episode 1, action 255\n"
     ]
    }
   ],
   "source": [
    "#Test memorize\n",
    "\n",
    "train_frames = []\n",
    "episodes = 2\n",
    "\n",
    "for i in range(episodes):\n",
    "\n",
    "    observation = env.reset()\n",
    "    done=False\n",
    "    agent.history.append(observation)\n",
    "    counter = 0\n",
    "    \n",
    "       \n",
    "    while not done:  \n",
    "        action = agent.act()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        agent.memorize(action, reward, observation, done)  \n",
    "        agent.learn()\n",
    "\n",
    "        train_frames.append({\n",
    "            'frame': env.render(mode='rgb_array'),\n",
    "            'state': observation,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            })\n",
    "        \n",
    "        if (counter%5 == 0):\n",
    "            print('Episode {}, action {}'.format(i, counter))\n",
    "        counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(train_frames[0]) # only call this once\n",
    "for frame in range train_frames[1:]:\n",
    "    img.set_data(env.render(frame) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T15:26:30.842178Z",
     "start_time": "2020-05-01T15:26:30.576682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7931b933d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gym' (namespace)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "reload(gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package gym:\n",
      "\n",
      "NAME\n",
      "    gym\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    gym (package)\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "FILE\n",
      "    (built-in)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
